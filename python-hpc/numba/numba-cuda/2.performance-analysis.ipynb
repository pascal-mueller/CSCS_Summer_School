{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the performance of the vecadd kernel\n",
    "\n",
    "\n",
    "In this section we will see how we can avoid the JIT cost of Numba, how we can measure the performance of the kernel without the `%timeit` magic, how we can use `nsys profile`, the CUDA profiler to analyze the performance of the kernel, and finally, we will evaluate the performance of the kernel.\n",
    "\n",
    "## Avoiding the JIT cost\n",
    "\n",
    "The previous exercise has shown that Numba will compile the CUDA kernel every time we call our program and, in order to amortize the compilation cost, we need several invocations. We would like to avoid this cost.\n",
    "\n",
    "Unlike the `@numba.jit` decorator, `@cuda.jit` does not accept a `cache` parameter, that would cache the generated code on the disk and use it on subsequent invocations of the program. Nonetheless, we can force the code generation at import time by supplying a function signature to the `@cuda.jit` decorator that describes the CUDA kernel. This will generate the CUDA code at the time when the decorator processes the function declaration and, therefore, we will avoid the runtime cost of JIT. Let's see how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "\n",
    "#@cuda.jit('void(Array(float64, 1, \"C\"), Array(float64, 1, \"C\"), Array(float64, 1, \"C\"))')\n",
    "@cuda.jit('void(float64[::1], float64[::1], float64[::1])')\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    '''The CUDA kernel'''\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This instructs the Numba runtime to compile the following function into a CUDA kernel (return type `void`) accepting three one-dimensional arrays of `float64` (or `double`) stored in row-major order (C convention). This way, Numba does not have to wait until the `_vecadd_cuda` function is called to figure out the argument types and compile the kernel. It can do this at import time, when it first encounters the function. The downside to that is that you can't call the function with a different type of arguments later. For more details on how you can specify function signatures in Numba, see [here](https://numba.readthedocs.io/en/latest/reference/types.html#numba-types).\n",
    "\n",
    "Let's retry our example now with this version of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random engine\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.uniform(size=N)\n",
    "y = rng.uniform(size=N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "%timeit -r2 -n 4 _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> Time the kernel with `%timeit -n1 -r1`. Try to increase the repetitions and experiment with different array sizes. What do you see?\n",
    "\n",
    "## Measuring the execution time of the kernel\n",
    "\n",
    "All you see from the previous exercise is the same execution time! What is happening? Actually, you are not measuring the kernel execution time, but rather the kernel launch time. CUDA kernels are launched asynchronously. This means that as soon as you launch the kernel on the GPU, the CPU will continue execution. In this case, it will continue executing and it will block at the statement that copies back the result to the host. \n",
    "\n",
    "How do we measure the kernel execution time then? For this, we are going to write a Python [context manager](https://docs.python.org/3.8/reference/datamodel.html?highlight=__getitem__#with-statement-context-managers) so as to measure the execution time of a region in a nice, Pythonic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_off = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + (self._t_end - self._t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about context managers, please refer elsewhere. Let's use our timer to time the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.uniform(size=N)\n",
    "y = rng.uniform(size=N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our timer seems to work fine; we still measure the kernel launch time as with `%timeit`. In order to measure the actual kernel execution time, we need to block the CPU calling thread until the kernel finishes, immediately after we launch the kernel. We can achieve that with `cuda.synchronize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.uniform(size=N)\n",
    "y = rng.uniform(size=N)\n",
    "z = np.empty_like(x)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "    cuda.synchronize()\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    np.add(x, y, out=z)\n",
    "\n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "print(f'Numpy time:       {t_ref.elapsed_time()} s')\n",
    "\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, the CUDA kernel is more than 100x faster than the numba vectorized implementation.\n",
    "\n",
    "Before analysing how good or bad this is, let's see an alternative way for measuring the kernel time that actually avoids the use of `cuda.synchronize()`.\n",
    "\n",
    "## Measuring the kernel execution time with CUDA events\n",
    "\n",
    "Inserting `cuda.synchronize()` without a reason could slow down your application, since it not only blocks the current CPU thread, but also imposes a synchronization point for all the CUDA streams on the GPU that are currently running in parallel.\n",
    "\n",
    "> A CUDA stream is essentially a series of sequential operations (data transfers, kernel launches, etc.) that execute on the GPU. Multiple CUDA streams may run independently on the GPU, thus allowing overlapping of operations, such as data transfers and execution of kernels.\n",
    "\n",
    "To avoid this, but also to obtain a more precise measurement, you can use [CUDA events](http://docs.nvidia.com/cuda/cuda-c-programming-guide/#events). You can imagine CUDA events as milestones associated with timestamps that you can insert between operations in a CUDA stream. Let's how we can adapt our `time_region` context manager to use CUDA events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_off = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + 1.e-3*cuda.event_elapsed_time(self._t_start, self._t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure a data region with CUDA events you first need to create two events: one for the start and one for the end. You can achieve that with the `cuda.event(timing=True)`. To start counting, you need to call `record()` on the starting event marking the \"arrival\" to that milestone. Similarly, you call `record()` on the ending event to mark the end of the region. Then you can obtain the elapsed time using the corresponding function as shown in the example above.\n",
    "\n",
    "Let's rewrite our vector addition example using the CUDA event timers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.uniform(size=N)\n",
    "y = rng.uniform(size=N)\n",
    "z = np.empty_like(x)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.to_device(z)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region_cuda() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    np.add(x, y, out=z)\n",
    "    \n",
    "with time_region_cuda() as t_copyout:\n",
    "    d_z.copy_to_host(z)\n",
    "        \n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "print(f'Numpy time:       {t_ref.elapsed_time()} s')\n",
    "\n",
    "\n",
    "# Copy back the result to the host\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the execution time obtained is the correct one without having to use `cuda.synchronize()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the performance of the kernel\n",
    "\n",
    "The question that arises is how good is the performance that we achieve. Let's inspect further the kernel. Each thread does two `float64` reads from the memory and one write and performs an addition. That means for one floating operation, the kernel must transfer to/from memory 24 bytes from the main memory. This gives us an  *arithmetic intensity* or *flop:byte ratio* of 0.0417. The lower this ratio is for a computational kernel, the more likely will be that the kernel is memory bandwidth bound. As the ratio increases, the kernel tends to be more compute bound. The theory behind the arithmetic intensity is covered by the *Roofline* performance model, which is outside the scope of this tutorial. For the moment, let's compute two performance metrics, the `Gflop/s` achieved by the kernel and the data transfer rate to/from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performance: {1.e-9*N/t_kernel.elapsed_time()} Gflop/s')\n",
    "print(f'Transfer rate: {1.e-9*3*N*8/t_kernel.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And for NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performance: {1.e-9*N/t_ref.elapsed_time()} Gflop/s')\n",
    "print(f'Transfer rate: {1.e-9*3*N*8/t_ref.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As you can, the GPU can deliver more than 100x bandwidth compared to the CPU. Checking the [datasheet](https://resources.nvidia.com/en-us-data-center-overview-mc/en-us-data-center-overview/grace-hopper-superchip-datasheet-partner) of the NVIDIA GH200 superchip, we can see that the peak nominal memory bandwidth is 4TB/s, meaning that our kernel utilizes 75-80% of the peak bandwidth.\n",
    "\n",
    "* Achieving the nominal peak memory bandwidth is usually not possible with real-life computational kernels, even with very low arithmetic intensity.  \n",
    "\n",
    "* <mark>NOTE</mark>: The numpy vector addition performance is not ideal, since it can't reach the memory bandwidth limit as it ought to. The problem could be related to CPU affinity issues, but we are not going to address them in this tutorial.For the Grace CPU, the effective memory bandwidth is ~500 GB/s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding the data transfer overhead\n",
    "\n",
    "So far we have only focused on the performance of the kernel. There is still a quite important topic we have not yet addressed. CUDA kernels require that the data they operate on is located on the device and we need to move that data there from the host. What is the cost of this data movement? Let's time our benchmark code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.uniform(size=N)\n",
    "y = rng.uniform(size=N)\n",
    "\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "with time_region_cuda() as t_copyin:\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "\n",
    "with time_region_cuda() as t_create:\n",
    "    d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region_cuda() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    z = x + y\n",
    "\n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "print(f'Numpy time:       {t_ref.elapsed_time()} s')\n",
    "\n",
    "\n",
    "# Copy back the result to the host\n",
    "with time_region_cuda() as t_copyout:\n",
    "    d_z.copy_to_host(res)\n",
    "\n",
    "print(f'Copyin time:  {t_copyin.elapsed_time()} s')  \n",
    "print(f'Create time:  {t_create.elapsed_time()} s')    \n",
    "print(f'Copyout time: {t_copyout.elapsed_time()} s')    \n",
    "\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data copy times are quite important! Data transfers is the No. 1 optimization that you should do when programming for the GPUs. You must minimize the data transfers to/from GPU by keeping the necessary data on the GPU for as long as it is needed.\n",
    "\n",
    "Before closing this discussion, let's see how fast is the data moved over to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Copyin rate: {1e-9*2*N*8/t_copyin.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bound by the data rate of the [NVLink-C2C](https://www.nvidia.com/en-us/data-center/nvlink-c2c/) connecting the CPU/GPU of the Grace Hopper superchip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Copyout rate: {1e-9*N*8/t_copyout.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling the CUDA code (optional)\n",
    "\n",
    "In this simple example of vector addition we assessed the performance and identified the bottlenecks ourselves, by analyzing the code structure and reasoning about it. In more complex codes or codes that you are not very familiar with, it would be good if this analysis could be done by a dedicated tool. Not to be misunderstood, understanding the code structure and its memory and compute requirements is essential for optimizing it in any case, but using a *performance profiler* is very handy for analyzing the performance bottlenecks, for helping you prioritizing your optimization targets and for understanding how much room for improvement exists.\n",
    "\n",
    "NVIDIA provides [Nsight Systems](https://developer.nvidia.com/nsight-systems) for profiling the code and inspecting the results.\n",
    "\n",
    "The `numba-cuda/src/vecadd.py` file contains the vector addition example as we have finally presented it here. Let's do a basic profing.\n",
    "\n",
    "First, we need to open a new terminal (\"File->New->Terminal\") and activate the virtual environment of the course's Python kernel.\n",
    "\n",
    "Now it's time to do our basic profiling:\n",
    "\n",
    "```bash\n",
    "cd numba-cuda/src\n",
    "nsys profile -o vecadd.nsys-rep python3 vecadd.py $((200*1000*1000))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
